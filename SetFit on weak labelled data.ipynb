{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af81f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 13:52:34.666472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-25 13:52:41.762535: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-25 13:52:41.762691: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-25 13:52:41.762705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer, sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01e24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the database object compatible with the model\n",
    "\n",
    "def make_huggingface_dataset(filename):\n",
    "    readfile = open(filename, 'r')\n",
    "    addresses = []\n",
    "    labels = []\n",
    "    for line in readfile:\n",
    "        addresses.append(line.split(',')[0].strip())\n",
    "        labels.append(int(line.split(',')[1].strip()))\n",
    "\n",
    "    # Create a Dataset object\n",
    "    dataset = Dataset.from_dict({\"idx\": range(len(addresses)), \"sentence\": addresses, \"label\": labels})\n",
    "\n",
    "    dset = Dataset.from_list(dataset)\n",
    "    \n",
    "    return dset\n",
    "\n",
    "#these files (train.csv, eval.csv) are generated by skweak\n",
    "t_dset = make_huggingface_dataset('train.csv')\n",
    "e_dset = make_huggingface_dataset('eval.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9862c85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 1754\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fbed43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['idx', 'sentence', 'label'],\n",
       "    num_rows: 439\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7871fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Applying column mapping to training dataset\n",
      "***** Running training *****\n",
      "  Num examples = 7016\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 439\n",
      "  Total train batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dd6e5cd6be452db4d6379280e7fd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24d4c8b4cf6400a901aa0811f6a3f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/439 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a SetFit model from Hub\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=t_dset,\n",
    "    eval_dataset=e_dset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    metric=\"accuracy\",\n",
    "    batch_size=16,\n",
    "    num_iterations=2, # The number of text pairs to generate for contrastive learning\n",
    "    num_epochs=1, # The number of epochs to use for contrastive learning\n",
    "    column_mapping={\"sentence\": \"text\", \"label\": \"label\"} # Map dataset columns to text/label expected by trainer\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707f28bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to evaluation dataset\n",
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9886104783599089}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c4e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
